{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARABISH - is an Arabic to English machine translation model \n",
    "\n",
    "Model based on the architecture of Transformer proposed by the famous paper ``Attention is all you need!``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Depencancies and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Tuple, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Preprocessing, Tokenization, and Prepartion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading Dataset from Hugging Face ☺\n",
    "\n",
    "CCMatrix Dataset is has been extracted from web crawls using the margin-based bitext mining techniques described at https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix.\n",
    "* 90 languages, 1,197 bitexts\n",
    "* total number of files: 90\n",
    "* total number of tokens: 112.14G\n",
    "* total number of sentence fragments: 7.37G\n",
    "\n",
    "* Languages\n",
    "Configs are generated for all language pairs in both directions. You can find the valid pairs in Homepage section of Dataset Description: https://opus.nlpl.eu/CCMatrix.php E.g.\n",
    "\n",
    "```bash\n",
    "print(next(iter(dataset['train'])))\n",
    "```\n",
    "\n",
    "```bash\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"score\": 1.2498379,\n",
    "        \"translation\": \n",
    "        {\n",
    "            \"en\": \"This uncertainty was very difficult for them.”\",\n",
    "            \"ar\": \"كانت حالة عدم اليقين هذه صعبة للغاية بالنسبة لهم.”\"\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8d49c8fef44dcfb531474f29df08f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744793c983724a1fa5d58c788a28ff49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36ac33fd203400faf580f410123972f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_raw = load_dataset(\"yhavinga/ccmatrix\", \"en-ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ar': 'وأقسم سبحانه وتعالى على تزكية نفسه صلى الله عليه وسلم وعصمتها من الآثام لمقامه الشريف، فالله سبحانه وتعالى زكى فؤاده ولسانه وجوارحه صلى الله عليه وسلم.', 'en': 'by [the plaintiff] in the [present lawsuit].‘‖ (Id. at p.'}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(ds_raw['train']))['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by [the plaintiff] in the [present lawsuit].‘‖ (Id. at p.\n",
      "وأقسم سبحانه وتعالى على تزكية نفسه صلى الله عليه وسلم وعصمتها من الآثام لمقامه الشريف، فالله سبحانه وتعالى زكى فؤاده ولسانه وجوارحه صلى الله عليه وسلم.\n",
      "----------------------------\n",
      "* I swear by [this] countryside,\n",
      "أقسم الله بهذا البلد الحرام ، وهو ( مكة ) .\n",
      "----------------------------\n",
      "Here in the earth all nations hate each other, and every one of them hates the Jew.\n",
      "هنا في الأرض جميع الأمم يكرهون بعضهم بعضا، وكل واحد منهم يكره اليهود.\n",
      "----------------------------\n",
      "Whom should I persuade (now again)\n",
      "من الذي ينبغي أن أقنعه (الآن مرة أخرى)\n",
      "----------------------------\n",
      "The left who founded your party once knew this.\"\n",
      "اليسار الذي أسس حزبك عرف ذلك مرة”.\n",
      "----------------------------\n",
      "This uncertainty was very difficult for them.”\n",
      "كانت حالة عدم اليقين هذه صعبة للغاية بالنسبة لهم.\"\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in ds_raw['train']:\n",
    "    print(item['translation']['en'])\n",
    "    print(item['translation']['ar'])\n",
    "    print('----------------------------')\n",
    "    i += 1\n",
    "    if i > 5: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make data tokenization functions ☺\n",
    "\n",
    "Using `tokenizers` and `datasets` from *Hugging Face*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/data_genarator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/data_genarator.py\n",
    "\n",
    "import datasets\n",
    "\n",
    "def data_genarator(dataset: datasets.dataset_dict.DatasetDict,\n",
    "                     lang: str):\n",
    "    \"\"\"\"\n",
    "    Genrate all sentences in a given dataset. \n",
    "    This function pass through the whole dataset rows as a genrator to yield every row in the translation for a spcific language to be processed\n",
    "    \n",
    "    Examples\n",
    "    Here are some examples of the inputs that are accepted::\n",
    "\n",
    "        genrator_dataset(dataset_raw, 'en')\n",
    "        genrator_dataset(dataset_raw, 'ar')\n",
    "\n",
    "\n",
    "    Args\n",
    "        dataset : :datasets.dataset_dict.DatasetDict\n",
    "            The Raw Dataset that should be iterated over.\n",
    "        lang: str\n",
    "            The Language argument in the dataset fields \n",
    "    Returns\n",
    "        iter(next(dataset['train]))\n",
    "    \"\"\"\n",
    "    for item in dataset:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/tokenizer.py\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def tokenizer() -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Function to create WordLevel Tokenizer. \n",
    "    The function has adds 4 special tokens for the dataset:\n",
    "        1- [UNK]: Unknown token for tokens that are not recognized in the dataset\n",
    "        2- [PAD]: Padding token to keep the size of sequance constant\n",
    "        3- [SOS]: Start Of Sentence token to indicate the sentance start\n",
    "        4- [EOS]: End Of Sentence token to indicaaate the sentence end\n",
    "\n",
    "    Example:\n",
    "        toeknizer = tokenizer()\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer: Tokenizer\n",
    "\n",
    "        A word-level tokenizer tokenizer. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "    tokenizer.pre_tokenizer =Whitespace()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/train_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/train_tokenizer.py\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from data_genarator import data_genarator\n",
    "import datasets\n",
    "\n",
    "def train_tokenizer(tokenizer: Tokenizer,\n",
    "                    dataset: datasets.dataset_dict.DatasetDict,\n",
    "                    lang: str) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Function to tokenize a certain dataset. \n",
    "    The function creates a WordLevelTrainer and train the tokeizer to the given dataset.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer\n",
    "            the tokenizer that should be trained\n",
    "        dataset: datasets.dataset_dict.DatasetDict\n",
    "            The dataset that should be tokenized\n",
    "        lang: str\n",
    "            The language of the tokenizer (This variable used only for naming)\n",
    "    \n",
    "    Example:\n",
    "        train_tokenizer(english_tokenizer, dataset_raw, 'en')\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer: Tokenizer\n",
    "\n",
    "        A tokenizer that already tokenized a certain dataset. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    trainer = WordLevelTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency=2)\n",
    "    tokenizer.train_from_iterator(data_genarator(dataset,lang), trainer=trainer)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/save_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/save_tokenizer.py\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "def save_tokenizer(tokenizer: Tokenizer,\n",
    "                   tokenizer_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Function to save a tokenizer in a json file.\n",
    "    The tokenizer be saved in a naming convention (tokenizername_language.json)\n",
    "    Args:\n",
    "        tokenizer: Tokenizer\n",
    "            The tokenizer to be saved \n",
    "        tokenizer_path: Path\n",
    "            the Path that the tokenizer should be saved it.\n",
    "    \n",
    "    Example:\n",
    "        tokenizer('tokenizer', 'en) ----> tokeinzer_en.json\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tokenizer.save(str(tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/make_or_load_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/make_or_load_tokenizer.py\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizer import tokenizer\n",
    "from train_tokenizer import train_tokenizer\n",
    "from save_tokenizer import save_tokenizer\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "\n",
    "def make_or_load_tokenizer(tokenizer_name:str, \n",
    "                           lang: str,\n",
    "                           dataset: datasets.dataset_dict.DatasetDict) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Function to build a WordLevel tokenizer, train it on a given dataset, and save it for later use.\n",
    "    if it already exits, just load it.\n",
    "\n",
    "    Args:\n",
    "        tokenizer_name: str\n",
    "            the name of the tokenizer file\n",
    "        lang: str\n",
    "            The language of the tokenizer (This variable used only for naming)\n",
    "        dataset: datasets.dataset_dict.DatasetDict\n",
    "            The dataset that should be tokenized\n",
    "        \n",
    "    Example:\n",
    "        make_or_load_tokenizer('tokenizer', 'en', ds_raw)\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer: Tokenizer\n",
    "\n",
    "        A tokenizer that already tokenized a certain dataset and saved for later use. \n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer_path = Path(tokenizer_name.format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = tokenizer()\n",
    "\n",
    "        train_tokenizer(tokenizer = tokenizer,\n",
    "                        dataset=dataset,\n",
    "                        lang=lang)\n",
    "        save_tokenizer(tokenizer=tokenizer,tokenizer_path=tokenizer_path)\n",
    "        \n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Makeing dataset loaders \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/causal_mask.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/causal_mask.py\n",
    "\n",
    "import torch\n",
    "def causal_mask(size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Function to provide a mask that masks or covers the future inputs and keep them away from the attention calculations.\n",
    "    Args:\n",
    "        size: int\n",
    "            the size of the mask \n",
    "    Returns: \n",
    "        mask: torch.Tensor\n",
    "            mask of True only at the future input (mask is squared (size*size) and has outer batch dimension)\n",
    "    \"\"\"\n",
    "    mask = torch.triu( torch.ones(size=(1,size,size)), diagonal=1).type(torch.int64)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/BilingualDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/BilingualDataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import datasets\n",
    "from tokenizers import Tokenizer\n",
    "from causal_mask import causal_mask\n",
    "class BilingualDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class inherit from ```bash torch.utils.data.Dataset``` to create encompase a raw data into a dataset\n",
    "    valid for use in dataloaders.\n",
    "    \n",
    "    The class has a constructor, __len__() method, and __getitem__() method.\n",
    "\n",
    "    The BilingualDataset class is ment to take tokenized data and store them as dataloaders,\n",
    "    it adds spical tokens to the raw tokens and keeps each sequance in a fixed constant length.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 datasaet: datasets.dataset_dict.DatasetDict,\n",
    "                 src_tokenizer: Tokenizer,\n",
    "                 trg_tokenizer: Tokenizer, \n",
    "                 seq_len: int):\n",
    "        \"\"\"\n",
    "        Constructor for the BilingualDataset class to create dataset instance.\n",
    "        the constructor saves the attuributes to each given instance and creates some attributes to be used.\n",
    "\n",
    "        Args:\n",
    "            dataset: a raw data set of any format like ```bash datasets.dataset_dict.DatasetDict```\n",
    "                the dataset should be in a format of bilingual data;\n",
    "                ```bash\n",
    "                {\n",
    "                    \"id\": 1,\n",
    "                    \"score\": 1.2498379,\n",
    "                    \"translation\": \n",
    "                    {\n",
    "                        \"en\": \"This uncertainty was very difficult for them.”\",\n",
    "                        \"ar\": \"كانت حالة عدم اليقين هذه صعبة للغاية بالنسبة لهم.”\"\n",
    "                    }\n",
    "                }\n",
    "                ``` \n",
    "            src_tokenizer: Tokenizer\n",
    "                the tokenizer should be used to tokenize the source language dataset\n",
    "            trg_tokenizer: Tokenizer\n",
    "                the tokenizer should be used to tokenize the target language dataset\n",
    "            seq_len: int\n",
    "                the maximum sequance length for every input or output.\n",
    "        Example:\n",
    "            dataset = BilingualDataset(raw_ds, tokenizer_en, tokenizer_ar, 200)\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.ds = datasaet\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.sos_token = torch.tensor([src_tokenizer.token_to_id('[SOS]')], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([src_tokenizer.token_to_id('[PAD]')], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([src_tokenizer.token_to_id('[EOS]')], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function to calculate the length of the dataset.\n",
    "        Args: None\n",
    "        Example:\n",
    "            BilingualDataset.__len__(ds)\n",
    "        Returns:\n",
    "            out: int\n",
    "                the number of rows in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self,\n",
    "                    index: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Function to retrive datarow from the dataset and tokenize it.\n",
    "\n",
    "        Args:\n",
    "            index: int\n",
    "                the inxed of the row should be tokenized.\n",
    "        \n",
    "        example:\n",
    "            BilingualDataset.__len__(ds, 5)\n",
    "\n",
    "        Returns:\n",
    "            out: Dict\n",
    "                a dictonary the containts the `encoder input tokens`, `decoder_input_tokens`,\n",
    "                `ecoder_mask` to mask the padding tokens and keep them away from computations.\n",
    "                `decoder_mask` to mask the padding tokens and the future tokens form the decoder input,\n",
    "                `label` the true output of the decoder, `src_text` the actual text without encoding, \n",
    "                `trg_text` the actual text after decodeing.\n",
    "\n",
    "        \"\"\"\n",
    "        src_txt, trg_txt = self.ds[index]['translation']\n",
    "        \n",
    "        enc_input_tokens = self.src_tokenizer.encode(src_txt).ids\n",
    "        dec_input_tokens = self.trg_tokenizer.encode(trg_txt).ids\n",
    "         \n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # 2 for SOS and EOS\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # 1 for SOS only\n",
    "        \n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "        \n",
    "        encoder_input = torch.cat(\n",
    "            tensors=[\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            tensors=[\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            tensors=[\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        self.mask = (torch.triu(torch.ones((1, decoder_input.size(0), decoder_input.size(0))), diagonal=1).type(torch.int64)) == 0\n",
    "\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "       \n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            'label': label,\n",
    "            'src_text': src_txt,\n",
    "            'tgt_text': trg_txt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/max_seq_len.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/max_seq_len.py\n",
    "import datasets\n",
    "from tokenizers import Tokenizer\n",
    "def calculate_max_seq_len(dataset: datasets.dataset_dict.DatasetDict,\n",
    "                          src_tokenizer: Tokenizer,\n",
    "                          trg_tokenizer: Tokenizer,\n",
    "                          src_lang: str,\n",
    "                          trg_lang: str,\n",
    "                          offset: int) -> int:\n",
    "    \"\"\"\n",
    "    Function to calculate the maximum allowable sequance length in the transformer acrhitecture.\n",
    "    it's calculated to be the longest sequance in the dataset + offset\n",
    "\n",
    "    Args:\n",
    "        dataset: datasets.dataset_dict.DatasetDict\n",
    "            the raw dataset to search through \n",
    "        src_tokenizer: Tokenizer\n",
    "            the tokenizer should be used to tokenize the source language dataset\n",
    "        trg_tokenizer: Tokenizer\n",
    "            the tokenizer should be used to tokenize the target language dataset \n",
    "        src_lang: str\n",
    "            the name of source language in the dataset\n",
    "        trg_lang: str\n",
    "            the name of target language in the dataset\n",
    "        offset: int\n",
    "            the number of offest above the max sequance in the dataset should be added to indicate max sequance\n",
    "    \n",
    "    Example:\n",
    "        calculate_max_seq_len(raw_ds, tokenizer_en, tokenizer_ar, 'en', 'ar', 10)\n",
    "\n",
    "    Returns: \n",
    "        out: int\n",
    "            the maximum allowable sequance length \n",
    "    \"\"\"\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "        \n",
    "    for item in dataset['train']:\n",
    "        src_tokens = src_tokenizer.encode(item['translation'][src_lang]).ids\n",
    "        trg_tokens = trg_tokenizer.encode(item['translation'][trg_lang]).ids\n",
    "        max_len_src = max(max_len_src, len(src_tokens))\n",
    "        max_len_tgt = max(max_len_tgt, len(trg_tokens))\n",
    "\n",
    "    print(f'Max Length of source sentence: {max_len_src}\\nMax Length of Target sentence: {max_len_tgt}')\n",
    "\n",
    "    return max(max_len_src,max_len_tgt) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/dataset_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/dataset_loader.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_dataset\n",
    "from make_or_load_tokenizer import make_or_load_tokenizer\n",
    "from BilingualDataset import BilingualDataset\n",
    "from max_seq_len import calculate_max_seq_len\n",
    "def dataset_loader(dataset_name: str,\n",
    "                 conf: Dict) -> Tuple(DataLoader, DataLoader, Tokenizer, Tokenizer):\n",
    "    \"\"\"\n",
    "    Function the loads the raw dataset, split it into train and validation, create tokenizers and tokenize it,\n",
    "    encopmase the data into PyTorch Dataset and turn it into dataloaders ready for training.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: str \n",
    "            the name of the Hugging Face dataset should be downloaded and loaded.\n",
    "        conf: Dict\n",
    "            configration of the datasets and tokenizers. Example:\n",
    "            ```bashconf= \n",
    "            {\n",
    "                'src_lang' : 'en',\n",
    "                'trg_lang` : 'ar',\n",
    "                'tokenizer_name: 'tokenizer',\n",
    "                'seq_len' : 200,\n",
    "                'batch_size': 8\n",
    "            }```\n",
    "\n",
    "    Examples:\n",
    "        tr_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = load_dataset(\"dataset\", config)\n",
    "    \n",
    "    Returns:\n",
    "        out: Tuple(DataLoader, DataLoader, Tokenizer, Tokenizer)\n",
    "            training dataloader, validation dataloader, source tokenizer, target tokenizer\n",
    "    \"\"\"\n",
    "    dataset_raw = load_dataset(dataset_name, f\"{conf['src_lang']}-{conf['trg_lang']}\")\n",
    "\n",
    "    tokenizer_src = make_or_load_tokenizer(tokenizer_name=conf['tokenizer_name'], \n",
    "                           lang=conf['src_lang'],\n",
    "                           dataset=dataset_raw)\n",
    "    tokenizer_trg = make_or_load_tokenizer(tokenizer_name=conf['tokenizer_name'], \n",
    "                           lang=conf['trg_lang'],\n",
    "                           dataset=dataset_raw)\n",
    "    \n",
    "\n",
    "    train_ds_size = int(0.9 * len(dataset_raw))\n",
    "    val_ds_size = len(dataset_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(dataset=dataset_raw, lengths=[train_ds_size, val_ds_size])\n",
    "    \n",
    "    conf['seq_len'] = calculate_max_seq_len(dataset=dataset_raw,\n",
    "                                            src_tokenizer=tokenizer_src,\n",
    "                                            trg_tokenizer=tokenizer_trg,\n",
    "                                            src_lang=conf['src_lang'],\n",
    "                                            trg_lang=conf['trg_lang'],\n",
    "                                            offset=20)\n",
    "    \n",
    "    train_dataset = BilingualDataset(datasaet=train_ds_raw,\n",
    "                                     src_tokenizer=tokenizer_src,\n",
    "                                     trg_tokenizer=tokenizer_trg,\n",
    "                                     seq_len=conf['seq_len'])\n",
    "    val_dataset = BilingualDataset(datasaet=val_ds_raw,\n",
    "                                     src_tokenizer=tokenizer_src,\n",
    "                                     trg_tokenizer=tokenizer_trg,\n",
    "                                     seq_len=conf['seq_len'])\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=conf['batch_size'],\n",
    "                                  shuffle=True)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                                  batch_size=1,\n",
    "                                  shuffle=False)\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_trg\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Transformer Components Model Building\n",
    "\n",
    "Building the Transformer model according to the famous paper *Attention is all you need* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
