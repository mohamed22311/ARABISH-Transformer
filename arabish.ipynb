{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARABISH - is an Arabic to English machine translation model \n",
    "\n",
    "Model based on the architecture of Transformer proposed by the famous paper ``Attention is all you need!``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Depencancies and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "\n",
    "from typing import List, Dict, Tuple, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Data Preprocessing, Tokenization, and Prepartion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Downloading Dataset from Hugging Face ☺\n",
    "\n",
    "CCMatrix Dataset is has been extracted from web crawls using the margin-based bitext mining techniques described at https://github.com/facebookresearch/LASER/tree/master/tasks/CCMatrix.\n",
    "* 90 languages, 1,197 bitexts\n",
    "* total number of files: 90\n",
    "* total number of tokens: 112.14G\n",
    "* total number of sentence fragments: 7.37G\n",
    "\n",
    "* Languages\n",
    "Configs are generated for all language pairs in both directions. You can find the valid pairs in Homepage section of Dataset Description: https://opus.nlpl.eu/CCMatrix.php E.g.\n",
    "\n",
    "```bash\n",
    "print(next(iter(dataset['train'])))\n",
    "```\n",
    "\n",
    "```bash\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"score\": 1.2498379,\n",
    "        \"translation\": \n",
    "        {\n",
    "            \"en\": \"This uncertainty was very difficult for them.”\",\n",
    "            \"ar\": \"كانت حالة عدم اليقين هذه صعبة للغاية بالنسبة لهم.”\"\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8d49c8fef44dcfb531474f29df08f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744793c983724a1fa5d58c788a28ff49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36ac33fd203400faf580f410123972f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_raw = load_dataset(\"yhavinga/ccmatrix\", \"en-ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ar': 'وأقسم سبحانه وتعالى على تزكية نفسه صلى الله عليه وسلم وعصمتها من الآثام لمقامه الشريف، فالله سبحانه وتعالى زكى فؤاده ولسانه وجوارحه صلى الله عليه وسلم.', 'en': 'by [the plaintiff] in the [present lawsuit].‘‖ (Id. at p.'}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(ds_raw['train']))['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by [the plaintiff] in the [present lawsuit].‘‖ (Id. at p.\n",
      "وأقسم سبحانه وتعالى على تزكية نفسه صلى الله عليه وسلم وعصمتها من الآثام لمقامه الشريف، فالله سبحانه وتعالى زكى فؤاده ولسانه وجوارحه صلى الله عليه وسلم.\n",
      "----------------------------\n",
      "* I swear by [this] countryside,\n",
      "أقسم الله بهذا البلد الحرام ، وهو ( مكة ) .\n",
      "----------------------------\n",
      "Here in the earth all nations hate each other, and every one of them hates the Jew.\n",
      "هنا في الأرض جميع الأمم يكرهون بعضهم بعضا، وكل واحد منهم يكره اليهود.\n",
      "----------------------------\n",
      "Whom should I persuade (now again)\n",
      "من الذي ينبغي أن أقنعه (الآن مرة أخرى)\n",
      "----------------------------\n",
      "The left who founded your party once knew this.\"\n",
      "اليسار الذي أسس حزبك عرف ذلك مرة”.\n",
      "----------------------------\n",
      "This uncertainty was very difficult for them.”\n",
      "كانت حالة عدم اليقين هذه صعبة للغاية بالنسبة لهم.\"\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for item in ds_raw['train']:\n",
    "    print(item['translation']['en'])\n",
    "    print(item['translation']['ar'])\n",
    "    print('----------------------------')\n",
    "    i += 1\n",
    "    if i > 5: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Make data tokenization functions ☺\n",
    "\n",
    "Using `tokenizers` and `datasets` from *Hugging Face*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/data_genarator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/data_genarator.py\n",
    "\n",
    "import datasets\n",
    "\n",
    "def data_genarator(dataset: datasets.dataset_dict.DatasetDict,\n",
    "                     lang: str):\n",
    "    \"\"\"\"\n",
    "    Genrate all sentences in a given dataset. \n",
    "    This function pass through the whole dataset rows as a genrator to yield every row in the translation for a spcific language to be processed\n",
    "    \n",
    "    Examples\n",
    "    Here are some examples of the inputs that are accepted::\n",
    "\n",
    "        genrator_dataset(dataset_raw, 'en')\n",
    "        genrator_dataset(dataset_raw, 'ar')\n",
    "\n",
    "\n",
    "    Args\n",
    "        dataset : :datasets.dataset_dict.DatasetDict\n",
    "            The Raw Dataset that should be iterated over.\n",
    "        lang: str\n",
    "            The Language argument in the dataset fields \n",
    "    Returns\n",
    "        iter(next(dataset['train]))\n",
    "    \"\"\"\n",
    "    for item in dataset:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/tokenizer.py\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "def tokenizer() -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Function to create WordLevel Tokenizer. \n",
    "    The function has adds 4 special tokens for the dataset:\n",
    "        1- [UNK]: Unknown token for tokens that are not recognized in the dataset\n",
    "        2- [PAD]: Padding token to keep the size of sequance constant\n",
    "        3- [SOS]: Start Of Sentence token to indicate the sentance start\n",
    "        4- [EOS]: End Of Sentence token to indicaaate the sentence end\n",
    "\n",
    "    Example:\n",
    "        toeknizer = tokenizer()\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer: Tokenizer\n",
    "\n",
    "        A word-level tokenizer tokenizer. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
    "    tokenizer.pre_tokenizer =Whitespace()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/train_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/train_tokenizer.py\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from data_genarator import data_genarator\n",
    "import datasets\n",
    "\n",
    "def train_tokenizer(tokenizer: Tokenizer,\n",
    "                    dataset: datasets.dataset_dict.DatasetDict,\n",
    "                    lang: str) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Function to tokenize a certain dataset. \n",
    "    The function creates a WordLevelTrainer and train the tokeizer to the given dataset.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: Tokenizer\n",
    "            the tokenizer that should be trained\n",
    "        dataset: datasets.dataset_dict.DatasetDict\n",
    "            The dataset that should be tokenized\n",
    "        lang: str\n",
    "            The language of the tokenizer (This variable used only for naming)\n",
    "    \n",
    "    Example:\n",
    "        train_tokenizer(english_tokenizer, dataset_raw, 'en')\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer: Tokenizer\n",
    "\n",
    "        A tokenizer that already tokenized a certain dataset. \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    trainer = WordLevelTrainer(special_tokens=['[UNK]', '[PAD]', '[SOS]', '[EOS]'], min_frequency=2)\n",
    "    tokenizer.train_from_iterator(data_genarator(dataset,lang), trainer=trainer)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/save_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/save_tokenizer.py\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "def save_tokenizer(tokenizer: Tokenizer,\n",
    "                   tokenizer_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Function to save a tokenizer in a json file.\n",
    "    The tokenizer be saved in a naming convention (tokenizername_language.json)\n",
    "    Args:\n",
    "        tokenizer: Tokenizer\n",
    "            The tokenizer to be saved \n",
    "        tokenizer_path: Path\n",
    "            the Path that the tokenizer should be saved it.\n",
    "    \n",
    "    Example:\n",
    "        tokenizer('tokenizer', 'en) ----> tokeinzer_en.json\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    tokenizer.save(str(tokenizer_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset/make_or_load_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/make_or_load_tokenizer.py\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizer import tokenizer\n",
    "from train_tokenizer import train_tokenizer\n",
    "from save_tokenizer import save_tokenizer\n",
    "from pathlib import Path\n",
    "import datasets\n",
    "\n",
    "def make_or_load_tokenizer(tokenizer_name:str, \n",
    "                           lang: str,\n",
    "                           dataset: datasets.dataset_dict.DatasetDict) -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Function to build a WordLevel tokenizer, train it on a given dataset, and save it for later use.\n",
    "    if it already exits, just load it.\n",
    "\n",
    "    Args:\n",
    "        tokenizer_name: str\n",
    "            the name of the tokenizer file\n",
    "        lang: str\n",
    "            The language of the tokenizer (This variable used only for naming)\n",
    "        dataset: datasets.dataset_dict.DatasetDict\n",
    "            The dataset that should be tokenized\n",
    "        \n",
    "    Example:\n",
    "        make_or_load_tokenizer('tokenizer', 'en', ds_raw)\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer: Tokenizer\n",
    "\n",
    "        A tokenizer that already tokenized a certain dataset and saved for later use. \n",
    "\n",
    "    \"\"\"\n",
    "    tokenizer_path = Path(tokenizer_name.format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer = tokenizer()\n",
    "\n",
    "        train_tokenizer(tokenizer = tokenizer,\n",
    "                        dataset=dataset,\n",
    "                        lang=lang)\n",
    "        save_tokenizer(tokenizer=tokenizer,tokenizer_path=tokenizer_path)\n",
    "        \n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Makeing dataset loaders \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/causal_mask.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/causal_mask.py\n",
    "\n",
    "import torch\n",
    "def causal_mask(size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Function to provide a mask that masks or covers the future inputs and keep them away from the attention calculations.\n",
    "    Args:\n",
    "        size: int\n",
    "            the size of the mask \n",
    "    Returns: \n",
    "        mask: torch.Tensor\n",
    "            mask of True only at the future input (mask is squared (size*size) and has outer batch dimension)\n",
    "    \"\"\"\n",
    "    mask = torch.triu( torch.ones(size=(1,size,size)), diagonal=1).type(torch.int64)\n",
    "    return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/BilingualDataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/BilingualDataset.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import datasets\n",
    "from tokenizers import Tokenizer\n",
    "from causal_mask import causal_mask\n",
    "class BilingualDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class inherit from ```bash torch.utils.data.Dataset``` to create encompase a raw data into a dataset\n",
    "    valid for use in dataloaders.\n",
    "    \n",
    "    The class has a constructor, __len__() method, and __getitem__() method.\n",
    "\n",
    "    The BilingualDataset class is ment to take tokenized data and store them as dataloaders,\n",
    "    it adds spical tokens to the raw tokens and keeps each sequance in a fixed constant length.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 datasaet: datasets.dataset_dict.DatasetDict,\n",
    "                 src_tokenizer: Tokenizer,\n",
    "                 trg_tokenizer: Tokenizer, \n",
    "                 seq_len: int):\n",
    "        \"\"\"\n",
    "        Constructor for the BilingualDataset class to create dataset instance.\n",
    "        the constructor saves the attuributes to each given instance and creates some attributes to be used.\n",
    "\n",
    "        Args:\n",
    "            dataset: a raw data set of any format like ```bash datasets.dataset_dict.DatasetDict```\n",
    "                the dataset should be in a format of bilingual data;\n",
    "                ```bash\n",
    "                {\n",
    "                    \"id\": 1,\n",
    "                    \"score\": 1.2498379,\n",
    "                    \"translation\": \n",
    "                    {\n",
    "                        \"en\": \"This uncertainty was very difficult for them.”\",\n",
    "                        \"ar\": \"كانت حالة عدم اليقين هذه صعبة للغاية بالنسبة لهم.”\"\n",
    "                    }\n",
    "                }\n",
    "                ``` \n",
    "            src_tokenizer: Tokenizer\n",
    "                the tokenizer should be used to tokenize the source language dataset\n",
    "            trg_tokenizer: Tokenizer\n",
    "                the tokenizer should be used to tokenize the target language dataset\n",
    "            seq_len: int\n",
    "                the maximum sequance length for every input or output.\n",
    "        Example:\n",
    "            dataset = BilingualDataset(raw_ds, tokenizer_en, tokenizer_ar, 200)\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.ds = datasaet\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = trg_tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.sos_token = torch.tensor([src_tokenizer.token_to_id('[SOS]')], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([src_tokenizer.token_to_id('[PAD]')], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([src_tokenizer.token_to_id('[EOS]')], dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Function to calculate the length of the dataset.\n",
    "        Args: None\n",
    "        Example:\n",
    "            BilingualDataset.__len__(ds)\n",
    "        Returns:\n",
    "            out: int\n",
    "                the number of rows in the dataset\n",
    "\n",
    "        \"\"\"\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self,\n",
    "                    index: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Function to retrive datarow from the dataset and tokenize it.\n",
    "\n",
    "        Args:\n",
    "            index: int\n",
    "                the inxed of the row should be tokenized.\n",
    "        \n",
    "        example:\n",
    "            BilingualDataset.__len__(ds, 5)\n",
    "\n",
    "        Returns:\n",
    "            out: Dict\n",
    "                a dictonary the containts the `encoder input tokens`, `decoder_input_tokens`,\n",
    "                `ecoder_mask` to mask the padding tokens and keep them away from computations.\n",
    "                `decoder_mask` to mask the padding tokens and the future tokens form the decoder input,\n",
    "                `label` the true output of the decoder, `src_text` the actual text without encoding, \n",
    "                `trg_text` the actual text after decodeing.\n",
    "\n",
    "        \"\"\"\n",
    "        src_txt, trg_txt = self.ds[index]['translation']\n",
    "        \n",
    "        enc_input_tokens = self.src_tokenizer.encode(src_txt).ids\n",
    "        dec_input_tokens = self.trg_tokenizer.encode(trg_txt).ids\n",
    "         \n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # 2 for SOS and EOS\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # 1 for SOS only\n",
    "        \n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "        \n",
    "        encoder_input = torch.cat(\n",
    "            tensors=[\n",
    "                self.sos_token,\n",
    "                torch.tensor(enc_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            tensors=[\n",
    "                self.sos_token,\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        label = torch.cat(\n",
    "            tensors=[\n",
    "                torch.tensor(dec_input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64)\n",
    "            ],\n",
    "            dim=0\n",
    "        )\n",
    "\n",
    "        self.mask = (torch.triu(torch.ones((1, decoder_input.size(0), decoder_input.size(0))), diagonal=1).type(torch.int64)) == 0\n",
    "\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "       \n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)),\n",
    "            'label': label,\n",
    "            'src_text': src_txt,\n",
    "            'tgt_text': trg_txt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/max_seq_len.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/max_seq_len.py\n",
    "import datasets\n",
    "from tokenizers import Tokenizer\n",
    "def calculate_max_seq_len(dataset: datasets.dataset_dict.DatasetDict,\n",
    "                          src_tokenizer: Tokenizer,\n",
    "                          trg_tokenizer: Tokenizer,\n",
    "                          src_lang: str,\n",
    "                          trg_lang: str,\n",
    "                          offset: int) -> int:\n",
    "    \"\"\"\n",
    "    Function to calculate the maximum allowable sequance length in the transformer acrhitecture.\n",
    "    it's calculated to be the longest sequance in the dataset + offset\n",
    "\n",
    "    Args:\n",
    "        dataset: datasets.dataset_dict.DatasetDict\n",
    "            the raw dataset to search through \n",
    "        src_tokenizer: Tokenizer\n",
    "            the tokenizer should be used to tokenize the source language dataset\n",
    "        trg_tokenizer: Tokenizer\n",
    "            the tokenizer should be used to tokenize the target language dataset \n",
    "        src_lang: str\n",
    "            the name of source language in the dataset\n",
    "        trg_lang: str\n",
    "            the name of target language in the dataset\n",
    "        offset: int\n",
    "            the number of offest above the max sequance in the dataset should be added to indicate max sequance\n",
    "    \n",
    "    Example:\n",
    "        calculate_max_seq_len(raw_ds, tokenizer_en, tokenizer_ar, 'en', 'ar', 10)\n",
    "\n",
    "    Returns: \n",
    "        out: int\n",
    "            the maximum allowable sequance length \n",
    "    \"\"\"\n",
    "\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "        \n",
    "    for item in dataset['train']:\n",
    "        src_tokens = src_tokenizer.encode(item['translation'][src_lang]).ids\n",
    "        trg_tokens = trg_tokenizer.encode(item['translation'][trg_lang]).ids\n",
    "        max_len_src = max(max_len_src, len(src_tokens))\n",
    "        max_len_tgt = max(max_len_tgt, len(trg_tokens))\n",
    "\n",
    "    print(f'Max Length of source sentence: {max_len_src}\\nMax Length of Target sentence: {max_len_tgt}')\n",
    "\n",
    "    return max(max_len_src,max_len_tgt) + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataset/dataset_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset/dataset_loader.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_dataset\n",
    "from make_or_load_tokenizer import make_or_load_tokenizer\n",
    "from BilingualDataset import BilingualDataset\n",
    "from max_seq_len import calculate_max_seq_len\n",
    "def dataset_loader(dataset_name: str,\n",
    "                 conf: Dict) -> Tuple(DataLoader, DataLoader, Tokenizer, Tokenizer):\n",
    "    \"\"\"\n",
    "    Function the loads the raw dataset, split it into train and validation, create tokenizers and tokenize it,\n",
    "    encopmase the data into PyTorch Dataset and turn it into dataloaders ready for training.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: str \n",
    "            the name of the Hugging Face dataset should be downloaded and loaded.\n",
    "        conf: Dict\n",
    "            configration of the datasets and tokenizers. Example:\n",
    "            ```bashconf= \n",
    "            {\n",
    "                'src_lang' : 'en',\n",
    "                'trg_lang` : 'ar',\n",
    "                'tokenizer_name: 'tokenizer',\n",
    "                'seq_len' : 200,\n",
    "                'batch_size': 8\n",
    "            }```\n",
    "\n",
    "    Examples:\n",
    "        tr_dataloader, val_dataloader, src_tokenizer, trg_tokenizer = load_dataset(\"dataset\", config)\n",
    "    \n",
    "    Returns:\n",
    "        out: Tuple(DataLoader, DataLoader, Tokenizer, Tokenizer)\n",
    "            training dataloader, validation dataloader, source tokenizer, target tokenizer\n",
    "    \"\"\"\n",
    "    dataset_raw = None\n",
    "    dataset_dir = Path(conf['dataset_dir'])\n",
    "    if dataset_dir.exists() and dataset_dir.is_dir():\n",
    "        # Load the dataset from the existing folder\n",
    "        print(\"Folder exists. Loading the dataset from the disk...\")\n",
    "        dataset_raw = load_from_disk(str(dataset_dir))\n",
    "    else:\n",
    "        print(\"Folder does not exist. Creating folder and downloading the dataset...\")\n",
    "        dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "        dataset_raw = load_dataset(conf['dataset_name'], f\"{conf['lang_src']}-{conf['lang_trg']}\")\n",
    "        dataset_raw.save_to_disk(str(dataset_dir))\n",
    "\n",
    "    tokenizer_src = make_or_load_tokenizer(tokenizer_name=conf['tokenizer_name'], \n",
    "                           lang=conf['src_lang'],\n",
    "                           dataset=dataset_raw)\n",
    "    tokenizer_trg = make_or_load_tokenizer(tokenizer_name=conf['tokenizer_name'], \n",
    "                           lang=conf['trg_lang'],\n",
    "                           dataset=dataset_raw)\n",
    "    \n",
    "\n",
    "    train_ds_size = int(0.9 * len(dataset_raw))\n",
    "    val_ds_size = len(dataset_raw) - train_ds_size\n",
    "    train_ds_raw, val_ds_raw = random_split(dataset=dataset_raw, lengths=[train_ds_size, val_ds_size])\n",
    "    \n",
    "    conf['seq_len'] = calculate_max_seq_len(dataset=dataset_raw,\n",
    "                                            src_tokenizer=tokenizer_src,\n",
    "                                            trg_tokenizer=tokenizer_trg,\n",
    "                                            src_lang=conf['src_lang'],\n",
    "                                            trg_lang=conf['trg_lang'],\n",
    "                                            offset=20)\n",
    "    \n",
    "    train_dataset = BilingualDataset(datasaet=train_ds_raw,\n",
    "                                     src_tokenizer=tokenizer_src,\n",
    "                                     trg_tokenizer=tokenizer_trg,\n",
    "                                     seq_len=conf['seq_len'])\n",
    "    val_dataset = BilingualDataset(datasaet=val_ds_raw,\n",
    "                                     src_tokenizer=tokenizer_src,\n",
    "                                     trg_tokenizer=tokenizer_trg,\n",
    "                                     seq_len=conf['seq_len'])\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                                  batch_size=conf['batch_size'],\n",
    "                                  shuffle=True)\n",
    "    val_dataloader = DataLoader(dataset=val_dataset,\n",
    "                                  batch_size=1,\n",
    "                                  shuffle=False)\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_trg\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Transformer Components Model Building\n",
    "\n",
    "Building the Transformer model according to the famous paper *Attention is all you need* \n",
    "\n",
    "The Transformer Has Four main parts: \n",
    "\n",
    "1- Encoders Set\n",
    "2- Decoders Set\n",
    "3- Input Netowrk \n",
    "4- Output Network\n",
    "\n",
    "The set of encoders contains N blocks of Encoder, Each has the following:\n",
    "\n",
    "* Self Attention block followed by a Residual Connection and Layer Normalization \n",
    "* Feed Forward block followed by a Residual Connection and Layer Normalization \n",
    "\n",
    "The set of Decoders contains N blocks of Decoder, Each has the following:\n",
    "\n",
    "* Masked Self Attention block followed by a Residual Connection and Layer Normalization \n",
    "* Cross Attention block followed by a Residual Connection and Layer Normalization\n",
    "* Feed Forward block followed by a Residual Connection and Layer Normalization \n",
    "\n",
    "The Input Network:\n",
    "\n",
    "* Tokenized inputs are mapped into an Embedding Matrix\n",
    "* Positional Encodings are added\n",
    "\n",
    "The Output Netowrk:\n",
    "\n",
    "* Linear Layer \n",
    "* Softmax Layer \n",
    "* Highest probabilites are mapped into the embedding materix to extract the tokens then the text\n",
    "\n",
    "For further information, I reccomend these resources:\n",
    "\n",
    "* https://machinelearningmastery.com/the-transformer-model/\n",
    "* https://medium.com/carbon-consulting transformer-architecture-how-transformer-models-work-46fc70b4ea59\n",
    "* https://www.youtube.com/watch?v=6JGzwI2pNfo&pp=ygUSdHJhbnNmb3JtZXIgYWJvYmty\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Building the Input Netowrk\n",
    "\n",
    "The input network has 2 compenent blocks (classes): \n",
    "\n",
    "* InputEmbeddings\n",
    "* PositionalEncoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/InputEmbeddings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/InputEmbeddings.py\n",
    "\n",
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 vocab_size: int):\n",
    "        \"\"\"\n",
    "        Class to create an embedding matrix of the size \n",
    "        of the vocabulary and the dimension vector for each token.\n",
    "        \n",
    "        Args:\n",
    "            d_model: int\n",
    "                The length of the vector to represnt each token\n",
    "            vocab_size: int\n",
    "                the number of tokens to embedded in the matrix\n",
    "\n",
    "        Example: \n",
    "            embed = InputEmbeddings(512, 10000)\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        function to embed each token passed through, it uses torch.nn.Embedding\n",
    "        Args:\n",
    "            x: torch.tensor\n",
    "                the token should be embeded\n",
    "        Returns:\n",
    "            the embeding vector \n",
    "            torch.tensor\n",
    "        \"\"\"\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/PositionalEncoding.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/PositionalEncoding.py\n",
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 seq_len: int,\n",
    "                 dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Class to create positional encodings and add them to the input sequance embedings.\n",
    "        The class creates a positional encoding matrix using the *Sinusoidal Positional Embedding Function*\n",
    "        ```bash\n",
    "        Embedding[i, 2k] = sin(position / (10000^(2k / d_model)))\n",
    "\n",
    "        Embedding[i, 2k+1] = cos(position / (10000^(2k / d_model)))\n",
    "        ```\n",
    "        then it takes a sequance that have been tokenized and retrived its embedings, and add positional encoding to the sequance (sequance only without padding).\n",
    "\n",
    "        Args:\n",
    "            d_model: int\n",
    "                the length of the embeding vector for each token\n",
    "            seq_len: int\n",
    "                the maximum allowable sequance length\n",
    "            dropout: float\n",
    "                the dropout precentage to avoid overfitting\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # positional encoding matrix in the shape of (seq_len, d_model)\n",
    "        pe_matrix = torch.zeros(seq_len, d_model)\n",
    "        \n",
    "        # create vector of shape (seq_len,1)\n",
    "        positions = torch.arange(0,seq_len, dtype=torch.float).unsqueeze_(1)\n",
    "        denominators = torch.pow(self=10000.0, \n",
    "                                 exponent= (2 * torch.arange(0, d_model//2) ) / d_model) # 10000^(2i/d_model), i is the index of embedding\n",
    "        # apply sin to even pos\n",
    "        pe[:,0::2] = torch.sin(positions/denominators) # sin(pos/10000^(2i/d_model))\n",
    "        # apply cos to odd pos\n",
    "        pe[:,1::2] = torch.cos(positions/denominators) # cos(pos/10000^(2i/d_model))\n",
    "\n",
    "        # add batch dimenshion  (1,seq_len,d_model) same dimension as the sequance embeddings (to be added over)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        self.register_buffer('pe',pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Function that adds positional encodings to the input embeddings only without the padding.\n",
    "        \"\"\"\n",
    "        x = x + (self.pe[:,:x.shape[1],:]).requires_grad_(False) \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoder and Decoder Blocks \n",
    "\n",
    "The Encoder and Decoder blocks include main components as following:\n",
    "\n",
    "* MultiHeadAttentionBlock\n",
    "* ResidualConnection\n",
    "* LayerNormalization \n",
    "* FeedForwardBlock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/MultiHeadAttentionBlock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/MultiHeadAttentionBlock.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 h: int,\n",
    "                 dropout: float) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Class that creates a Multihead Attention explained in the paper `Attention is all you need`\n",
    "        \n",
    "        The math function for it is:\n",
    "        \n",
    "        ```bash\n",
    "        Head(i) = Attention(QW_q, KW_k, VW_v)\n",
    "\n",
    "        MultiHead(Q,K,V) = Concat(head(1), head(2), ...., head(h))W_o\n",
    "        ```\n",
    "\n",
    "        self attention is being computed (i.e., query, key, and value are the same tensor).\n",
    "        inputs are batched (3D) with batch_first==True\n",
    "\n",
    "        Args:\n",
    "            d_model: int \n",
    "                the length of the embeding vector for each token\n",
    "            h: int\n",
    "                Number of heads \n",
    "            dropout: float\n",
    "                the dropout precentage to avoid overfitting\n",
    "            \n",
    "        Returns: \n",
    "            out: torch.Tensor\n",
    "                The Multihead Attention\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        assert d_model % h == 0, 'd_model is not divisble by h'\n",
    "        self.d_k = d_model // h  # d_k is the length of each head\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False) # Query weight matrix\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False) # Key weigth matrix\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False) # Value weight matrix\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False) # Output weight matrix \n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query: torch.Tensor, \n",
    "                  key: torch.Tensor, \n",
    "                  value: torch.Tensor, \n",
    "                  mask: torch.Tensor,\n",
    "                  dropout: nn.Dropout):\n",
    "        \"\"\"\n",
    "        Function to calculate the attnetion process.\n",
    "\n",
    "        Args:\n",
    "            query: torch.Tensor\n",
    "                The Query embeddings of shape (batch, h, seq_len, d_k)\n",
    "                Queries are compared against key-value pairs to produce the output. \n",
    "                See “Attention Is All You Need” for more details.\n",
    "            \n",
    "            key: torch.Tensor\n",
    "                Key embeddings of shape (batch, h, seq_len, d_k)\n",
    "\n",
    "            value: torch.Tensor\n",
    "                Value embeddings of shape  (batch, h, seq_len, d_k)\n",
    "\n",
    "            mask: torch.Tensor\n",
    "                 If specified, a mask of shape (batch, h, seq_len) indicating which elements within key to ignore \n",
    "                 for the purpose of attention (i.e. treat as “padding”). \n",
    "                 Binary masks are supported. For a binary mask, a True value indicates that \n",
    "                 the corresponding key value will be ignored for the purpose of attention.\n",
    "\n",
    "            dropout: nn.Dropout \n",
    "                The dropout layer to drop some weights randomly from calculations.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        d_k = query.shape[-1] \n",
    "\n",
    "        # Attention_scores shape:  (batch, h, seq_len, seq_len)\n",
    "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None: \n",
    "            attention_scores.masked_fill_(mask==0, -1e9)\n",
    "\n",
    "        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len)\n",
    "\n",
    "        if dropout is not None: \n",
    "            attention_scores = dropout(attention_scores)\n",
    "\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self,\n",
    "                q: torch.Tensor,\n",
    "                k: torch.Tensor, \n",
    "                v: torch.Tensor, \n",
    "                mask: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Fucntion calculates the multihead attention.\n",
    "        \"\"\"\n",
    "        # (batch, seq_len, d_model)  same dimension in and out\n",
    "        query = self.w_q(q) \n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        # Devide it into heads where the length of each head is d_k = d_model//h\n",
    "        # (batch, seq_len, d_model) ---> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
    "        query = query.view(query.shape[0],query.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0],key.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0],value.shape[1], self.h, self.d_k).transpose(1,2)\n",
    "\n",
    "        head_attention, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # (batch, h, seq_len, d_k) ---> (batch, seq_len, h, d_k)---> (batch, seq_len, d_model)\n",
    "        head_attention = head_attention.transpose(1,2).contiguous().view(head_attention.shape[0], -1, self.h * self.d_k)\n",
    "        \n",
    "        return self.w_o(head_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/LayerNormalization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/LayerNormalization.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies Layer Normalization over a mini-batch of inputs.\n",
    "\n",
    "    This layer implements the operation as described in the paper *Layer Normalization*\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 eps: float = 10**-6):\n",
    "        \n",
    "        \"\"\"\n",
    "        Applies Layer Normalization over a mini-batch of inputs.\n",
    "\n",
    "        This layer implements the operation as described in the paper *Layer Normalization*\n",
    "\n",
    "        Args:\n",
    "            eps: float\n",
    "                a value added to the denominator for numerical stability. Default: 1e-6\n",
    "\n",
    "        Variabels:\n",
    "            alpha: the learnable weights of the module\n",
    "            bias: the learnable bias of the module\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # multliplicative \n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # addatitive \n",
    "\n",
    "    \n",
    "    def forward(self,x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Apply Layer Normalization by calculating the mean and STD over the layer, \n",
    "        and apply the normalization equation:\n",
    "        ```bash\n",
    "        layer_norm = Alpha * X` / (std + bais + eps)\n",
    "        ``` \n",
    "        \"\"\"\n",
    "        mean = x.mean(dim= -1, keepdim=True) # keepdim=True -> the mean function cancel dim that's applied to but keepdim doesn't \n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model/ResidualConnection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/ResidualConnection.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from LayerNormalization import LayerNormalization\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to create a Residual Connection to add the input to previous layers to thier outputs.\n",
    "    \n",
    "    The idea of Residual Connection came from ResNets. \n",
    "\n",
    "    ResNet networks are characterized by skip connections, or shortcuts to jump over some layers, \n",
    "    this trick gives the ability to train really deep networks without caring about \n",
    "    The problem of gradient vanishing.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dropout: float)->None:\n",
    "        \"\"\"\n",
    "        Class to create a Residual Connection to add the input to previous layers to thier outputs.\n",
    "    \n",
    "        The idea of Residual Connection came from ResNets. \n",
    "\n",
    "        ResNet networks are characterized by skip connections, or shortcuts to jump over some layers, \n",
    "        this trick gives the ability to train really deep networks without caring about \n",
    "        The problem of gradient vanishing.\n",
    "\n",
    "        Args: \n",
    "            dropout: float\n",
    "                the dropout precentage to avoid overfitting\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        Function makes skip connection and apply layer normlaization. \n",
    "        \"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/FeedForwardBlock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/FeedForwardBlock.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Class the creates Feed Forward Netowrk, just simple sturcture of two Linear layers and some dropout.\n",
    "\n",
    "        Args:\n",
    "            d_model: int \n",
    "                the length of the embeding vector for each token\n",
    "            dropout: float\n",
    "                the dropout precentage to avoid overfitting\n",
    "            d_ff: int\n",
    "                number of neurons in the first layer.             \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=d_model, out_features=d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=d_ff, out_features=d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, dff) --> (batch, seq_len, d_model)\n",
    "        return self.layer_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/EncoderBlock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/EncoderBlock.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ResidualConnection import ResidualConnection\n",
    "from MultiHeadAttentionBlock import MultiHeadAttentionBlock\n",
    "from FeedForwardBlock import FeedForwardBlock\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 self_attention_block: MultiHeadAttentionBlock,\n",
    "                 feed_forward_block: FeedForwardBlock,\n",
    "                 dropout: float) -> None:\n",
    "        \"\"\"\n",
    "        Class to define an Encoder Block. \n",
    "        The architecture prposed in the paper *Attention is all you need*\n",
    "\n",
    "        The Encoder block contains a self_attention block, Residual Connection, LayerNormalization, and FeedForward.\n",
    "\n",
    "        Args:\n",
    "            self_attention_block: MultiHeadAttentionBlock\n",
    "                Block that calculates the attention scores of each to token to the other tokens in the sequance.\n",
    "\n",
    "            feed_forward_block: FeedForwardBlock\n",
    "                Linear Network\n",
    "\n",
    "            dropout: float\n",
    "                the dropout precentage to avoid overfitting\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x, src_mask))\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/Encoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/Encoder.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from LayerNormalization import LayerNormalization\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layers: nn.ModuleList) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Class that creates a number of Encoder blocks. \n",
    "        \n",
    "        Args:\n",
    "            layers: nn.ModuleList\n",
    "                list of encoder blocks\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/DecoderBlock.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/DecoderBlock.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from MultiHeadAttentionBlock import MultiHeadAttentionBlock\n",
    "from FeedForwardBlock import FeedForwardBlock\n",
    "from ResidualConnection import ResidualConnection\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 self_attention_block: MultiHeadAttentionBlock,\n",
    "                 cross_attention_block: MultiHeadAttentionBlock,\n",
    "                 feed_forward_block: FeedForwardBlock,\n",
    "                 dropout: float) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Class to define an Decoder Block. \n",
    "        The architecture prposed in the paper *Attention is all you need*\n",
    "\n",
    "        The Decoder block contains a self_attention block, cross_attention block, \n",
    "        Residual Connection, LayerNormalization, and FeedForward.\n",
    "\n",
    "        Args:\n",
    "            self_attention_block: MultiHeadAttentionBlock\n",
    "                Multihead Attetion to calculate the attention in the decoder input sequance.\n",
    "\n",
    "            cross_attention_block: MultiHeadAttentionBlock\n",
    "                Multihead Attetion to calculate the attention between the Encoder output and Decoder input\n",
    "\n",
    "            feed_forward_block: FeedForwardBlock\n",
    "                Linear Network\n",
    "\n",
    "            dropout: float\n",
    "                the dropout precentage to avoid overfitting\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__() \n",
    "\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # 3 Residual Connections \n",
    "    \n",
    "    def forward(self, x, econder_output, src_mask, tgt_mask):\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x,x,x,tgt_mask))\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x,econder_output, econder_output,src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/Decoder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/Decoder.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from LayerNormalization import LayerNormalization\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 layers: nn.ModuleList) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        Class that creates a number of Decoder blocks. \n",
    "        \n",
    "        Args:\n",
    "            layers: nn.ModuleList\n",
    "                list of decoder blocks\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Building the Output Netowrk\n",
    "\n",
    "The input network has one compenent block : \n",
    "\n",
    "* Projection Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/ProjectionLayer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/ProjectionLayer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 vocab_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Class of projection layer -> a linear layer followed by a softmax to \n",
    "        output the probability of each token. \n",
    "\n",
    "        Args:\n",
    "            d_model: int\n",
    "                The length of the vector to represnt each token\n",
    "            vocab_size: int\n",
    "                the number of tokens to embedded in the matrix\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # (Batch, seq_len, d_model) ---> (batch, seq_len, vocab_size)\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building the Transformer \n",
    "\n",
    "We build the transformer class by collection all these blocks to work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/Transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/Transformer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from Encoder import Encoder\n",
    "from Decoder import Decoder\n",
    "from InputEmbeddings import InputEmbeddings\n",
    "from PositionalEncoding import PositionalEncoding\n",
    "from ProjectionLayer import ProjectionLayer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder: Encoder,\n",
    "                 decoder: Decoder,\n",
    "                 src_embed: InputEmbeddings,\n",
    "                 trg_embed: InputEmbeddings,\n",
    "                 src_pos: PositionalEncoding,\n",
    "                 trg_pos: PositionalEncoding,\n",
    "                 projection_layer: ProjectionLayer):\n",
    "        \n",
    "        \"\"\"\n",
    "        A transformer model.\n",
    "\n",
    "        User is able to modify the attributes as needed. \n",
    "        The architecture is based on the paper “Attention Is All You Need”. \n",
    "        Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\n",
    "        \n",
    "        Args:\n",
    "            encoder: Encoder\n",
    "                Encoder that encodes the input tokens \n",
    "            decoder: Decoder\n",
    "                Decoder to decode the encoder output into tokens \n",
    "            src_embed: InputEmbeddings\n",
    "                The embedding matrix for the source inputs \n",
    "            trg_embed: InputEmbeddings\n",
    "                the embeding matrix for the target inputs \n",
    "            src_pos: PositionalEncoding\n",
    "                the source positional encodings \n",
    "            trg_pos: PositionalEncoding\n",
    "                the target positional encodings \n",
    "            projection_layer: ProjectionLayer\n",
    "                layer to project the decoder output into tokens \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.trg_embed = trg_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.trg_pos = trg_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Function that calculates the input embeddings then encodes them and add positional encoding.\n",
    "        \"\"\"\n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, encoder_output, src_mask, trg, trg_mask):\n",
    "        \"\"\"\n",
    "        Function to decode the encoder output\n",
    "        \"\"\"\n",
    "        trg = self.trg_embed(trg)\n",
    "        trg = self.trg_pos(trg)\n",
    "        return self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
    "    \n",
    "    def project(self, x):\n",
    "        \"\"\"\n",
    "        Function that project the decoder output to tokens \n",
    "        \"\"\"\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Model Builder Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model/build_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model/build_transformer.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from InputEmbeddings import InputEmbeddings\n",
    "from Transformer import Transformer\n",
    "from PositionalEncoding import PositionalEncoding\n",
    "from MultiHeadAttentionBlock import MultiHeadAttentionBlock\n",
    "from DecoderBlock import DecoderBlock\n",
    "from EncoderBlock import EncoderBlock\n",
    "from Encoder import Encoder\n",
    "from Decoder import Decoder\n",
    "from FeedForwardBlock import FeedForwardBlock\n",
    "from ProjectionLayer import ProjectionLayer\n",
    "\n",
    "\n",
    "def build_transformer(src_vocab_size: int,\n",
    "                      trg_vocab_size: int,\n",
    "                      src_seq_len: int,\n",
    "                      trg_seq_len: int,\n",
    "                      d_model: int = 512,\n",
    "                      N: int = 6,\n",
    "                      h: int = 8,\n",
    "                      dropout: float = 0.1,\n",
    "                      d_ff: int = 2048) -> Transformer:\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that build a transformer model. \n",
    "\n",
    "    Args:\n",
    "        src_vocab_size: int\n",
    "            Size of the source vocab \n",
    "        \n",
    "        trg_vocab_size: int\n",
    "            Size of the target vocab\n",
    "\n",
    "        src_seq_len: int\n",
    "            Maximum sequance length for the source inputs \n",
    "        \n",
    "        trg_seq_len: int\n",
    "            Maximum sequance length for the target inputs \n",
    "\n",
    "        d_model: int = 512\n",
    "            Size of the embedding vector for each token in the embedding matrix\n",
    "        \n",
    "        N: int = 6\n",
    "            Number of blocks in the Encoder and Decoder \n",
    "        \n",
    "        h: int = 8\n",
    "            Number of Heads in the Multihead attetion blocks\n",
    "\n",
    "        dropout: float = 0.1\n",
    "            Dropout precentage to drop while calculaations randomly to avoid overfitting\n",
    "                     \n",
    "        d_ff: int = 2048\n",
    "            Number of hidden neourns in the projection layer\n",
    "    \n",
    "    Example:\n",
    "        transformer = build_transformer(1000, 1000, 300, 300)\n",
    "\n",
    "    Returns:\n",
    "        transformer: Transformer\n",
    "    \"\"\"\n",
    "    # Create the embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size)\n",
    "    trg_embed = InputEmbeddings(d_model, trg_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    trg_pos = PositionalEncoding(d_model, trg_seq_len, dropout)\n",
    "\n",
    "    # Create the encoder blocks\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    # Create the deocder blocks\n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model,h,dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "    # Create the encoder and decoder \n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, trg_vocab_size)\n",
    "\n",
    "    # Create the transformer\n",
    "    transformer = Transformer(encoder, decoder, src_embed, trg_embed, src_pos, trg_pos, projection_layer)\n",
    "\n",
    "    # Intialize the parameters using Xavier intialization \n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/greedy_decode.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/greedy_decode.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from model.Transformer import Transformer\n",
    "from dataset.causal_mask import causal_mask\n",
    "\n",
    "def greedy_decode(model: Transformer, \n",
    "                  source_tokens: torch.Tensor,\n",
    "                  source_mask: torch.Tensor,\n",
    "                  tokenizer_src: Tokenizer,\n",
    "                  tokenizer_trg: Tokenizer,\n",
    "                  max_len: int,\n",
    "                  device: torch.device):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that calculates the output of the transformer in greedy way.\n",
    "    (output the hieghts probability only)\n",
    "\n",
    "    Args:\n",
    "        model: Transformer\n",
    "            Model that should used for inference \n",
    "        \n",
    "        source_tokens: torch.Tensor\n",
    "            the input sequance ids \n",
    "        \n",
    "        source_mask: torch.Tensor\n",
    "            Mask for the input size to avoid calculations for paddings \n",
    "        \n",
    "        tokenizer_src: Tokenizer\n",
    "            the tokenizer used in the source language\n",
    "        \n",
    "        tokenizer_trg: Tokenizer\n",
    "            the tokenizer used in the target language\n",
    "        \n",
    "        max_len: int\n",
    "            the maximum sequance length allowed\n",
    "\n",
    "        device: torch.device\n",
    "            the hardware device that's used in the compuations\n",
    "        \n",
    "    Example:\n",
    "        model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "    Returns:\n",
    "        out: torch.Tensor\n",
    "            a sequance of the highest probabities. \n",
    "    \"\"\"\n",
    "    \n",
    "    sos_idx = tokenizer_trg.token_to_id('[SOS]') # Start of sentence id (each token has id in the tokenizer)\n",
    "    eos_idx = tokenizer_trg.token_to_id('[EOS]') # End of sentence id\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every output predication\n",
    "    encoder_output = model.encode(source_tokens, source_mask)\n",
    "\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source_tokens).to(device)\n",
    "\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # build mask for target\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # calculate output\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # get next token\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            tensors=[\n",
    "                decoder_input, \n",
    "                torch.empty(1, 1).type_as(source_tokens).fill_(next_word.item()).to(device)\n",
    "            ], \n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train/beam_search_decode.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/beam_search_decode.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from model.Transformer import Transformer\n",
    "from dataset.causal_mask import causal_mask\n",
    "\n",
    "def beam_search_decode(model: Transformer,\n",
    "                       beam_size: int,\n",
    "                       source_tokens: torch.Tensor,\n",
    "                       source_mask: torch.Tensor,\n",
    "                       tokenizer_src: Tokenizer,\n",
    "                       tokenizer_trg: Tokenizer,\n",
    "                       max_len: int,\n",
    "                       device: torch.device):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that calculates the multible candidate output of the transformer to choose from.\n",
    "    (output top `beam_size` hieghts probabilities)\n",
    "\n",
    "    Args:\n",
    "        model: Transformer\n",
    "            Model that should used for inference \n",
    "        \n",
    "        beam_size: int\n",
    "            Number to indicate how many candidates to consider     \n",
    "\n",
    "        source_tokens: torch.Tensor\n",
    "            the input sequance ids \n",
    "        \n",
    "        source_mask: torch.Tensor\n",
    "            Mask for the input size to avoid calculations for paddings \n",
    "        \n",
    "        tokenizer_src: Tokenizer\n",
    "            the tokenizer used in the source language\n",
    "        \n",
    "        tokenizer_trg: Tokenizer\n",
    "            the tokenizer used in the target language\n",
    "        \n",
    "        max_len: int\n",
    "            the maximum sequance length allowed\n",
    "\n",
    "        device: torch.device\n",
    "            the hardware device that's used in the compuations\n",
    "        \n",
    "    Example:\n",
    "        model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "    Returns:\n",
    "        out: List\n",
    "            List of sequances that are candidates. \n",
    "    \"\"\"\n",
    "    \n",
    "    sos_idx = tokenizer_trg.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_trg.token_to_id('[EOS]')\n",
    "\n",
    "    # Precompute the encoder output and reuse it for every step\n",
    "    encoder_output = model.encode(source_tokens, source_mask)\n",
    "\n",
    "    # Initialize the decoder input with the sos token\n",
    "    decoder_initial_input = torch.empty(1, 1).fill_(sos_idx).type_as(source_tokens).to(device)\n",
    "\n",
    "    # Create a candidate list\n",
    "    candidates = [(decoder_initial_input, 1)]\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # If a candidate has reached the maximum length, it means we have run the decoding for at least max_len iterations, so stop the search\n",
    "        if any([cand.size(1) == max_len for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "        # Create a new list of candidates\n",
    "        new_candidates = []\n",
    "\n",
    "        for candidate, score in candidates:\n",
    "\n",
    "            # Do not expand candidates that have reached the eos token\n",
    "            if candidate[0][-1].item() == eos_idx:\n",
    "                continue\n",
    "\n",
    "            # Build the candidate's mask\n",
    "            candidate_mask = causal_mask(candidate.size(1)).type_as(source_mask).to(device)\n",
    "            # calculate output\n",
    "            out = model.decode(encoder_output, source_mask, candidate, candidate_mask)\n",
    "            # get next token probabilities\n",
    "            prob = model.project(out[:, -1])\n",
    "            # get the top k candidates\n",
    "            topk_prob, topk_idx = torch.topk(prob, beam_size, dim=1)\n",
    "            for i in range(beam_size):\n",
    "                # for each of the top k candidates, get the token and its probability\n",
    "                token = topk_idx[0][i].unsqueeze(0).unsqueeze(0)\n",
    "                token_prob = topk_prob[0][i].item()\n",
    "                # create a new candidate by appending the token to the current candidate\n",
    "                new_candidate = torch.cat([candidate, token], dim=1)\n",
    "                # We sum the log probabilities because the probabilities are in log space\n",
    "                new_candidates.append((new_candidate, score + token_prob))\n",
    "\n",
    "        # Sort the new candidates by their score\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
    "        # Keep only the top k candidates\n",
    "        candidates = candidates[:beam_size]\n",
    "\n",
    "        # If all the candidates have reached the eos token, stop\n",
    "        if all([cand[0][-1].item() == eos_idx for cand, _ in candidates]):\n",
    "            break\n",
    "\n",
    "    # Return the best candidate\n",
    "    return candidates[0][0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/run_validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/run_validation.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "from tokenizers import Token\n",
    "\n",
    "from model.Transformer import Transformer\n",
    "from train.greedy_decode import greedy_decode\n",
    "from train.beam_search_decode import beam_search_decode\n",
    "from evaluate.evaluate_model_outputs import evaluate_model_outputs\n",
    "\n",
    "def run_validation(model: Transformer,\n",
    "                   validation_ds: DataLoader,\n",
    "                   tokenizer_src: Tokenizer,\n",
    "                   tokenizer_trg: Tokenizer,\n",
    "                   max_len: int, \n",
    "                   device: torch.device,\n",
    "                   print_msg: function,\n",
    "                   global_step: int,\n",
    "                   writer: tensorboard,\n",
    "                   beam_size: int=1\n",
    "                   num_examples: int=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to make predictions on the validation set to test the model performance.\n",
    "    \n",
    "    The function also evaluate the preidctions useing *weights&biases* , *torchmetrics*,\n",
    "    and also *tensorboard*.\n",
    "\n",
    "    Args:\n",
    "        model: Transformer\n",
    "            Model that should used for inference \n",
    "        \n",
    "        validation_ds: Dataloader\n",
    "            the validation dataloader to be used for validation\n",
    "\n",
    "        tokenizer_src: Tokenizer\n",
    "            the tokenizer used in the source language\n",
    "        \n",
    "        tokenizer_trg: Tokenizer\n",
    "            the tokenizer used in the target language\n",
    "        \n",
    "        max_len: int\n",
    "            the maximum sequance length allowed\n",
    "\n",
    "        device: torch.device\n",
    "            the hardware device that's used in the compuations\n",
    "\n",
    "        print_msg: function\n",
    "            function to create a message to appear the at the TQDM bar while training the model\n",
    "        \n",
    "        global_step: int \n",
    "            variable used to idicate the state globally and used for resuming training\n",
    "\n",
    "        writer: tensorboard:\n",
    "            tensorboard writer used in evaluation\n",
    "\n",
    "        beam_size: int\n",
    "            if you want beam search -> Number to indicate how many candidates to consider  \n",
    "            if you want greedy serach -> beam_size = 1 (default = 1)\n",
    "    \n",
    "        num_examples: int \n",
    "            Number of samples in the validation data to be tested (default=2)\n",
    "\n",
    "    Example:\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_trg, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval() # Put the model in the evaluation mode\n",
    "\n",
    "    count = 0 # counter to break when the num_examples reached\n",
    "    source_texts = [] # List to store the source input text for each sample used in the validation \n",
    "    expected = [] # List to store the true output text for each sample used in the validation \n",
    "    predicted = [] # List to store the predicted output text for each sample used in the validation \n",
    "\n",
    "    try:\n",
    "        # get the console window width\n",
    "        with os.popen('stty size', 'r') as console:\n",
    "            _, console_width = console.read().split()\n",
    "            console_width = int(console_width)\n",
    "    except:\n",
    "        # If we can't get the console width, use 80 as default\n",
    "        console_width = 80\n",
    "\n",
    "    with torch.no_grad(): # stop calculating the gradients while testing\n",
    "\n",
    "        for batch in validation_ds: # iterate over each batch in the validation set ot calculate the result\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            encoder_input = batch[\"encoder_input\"].to(device) # (batch, seq_len)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device) # (batch, 1, 1, seq_len)\n",
    "\n",
    "            # check that the batch size is 1\n",
    "            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n",
    "\n",
    "            model_out = None\n",
    "\n",
    "            if beam_size > 1:\n",
    "                model_out = beam_search_decode(model=model,\n",
    "                       beam_size=beam_size,\n",
    "                       source_tokens=encoder_input,\n",
    "                       source_mask=encoder_mask,\n",
    "                       tokenizer_src=tokenizer_src,\n",
    "                       tokenizer_trg=tokenizer_trg,\n",
    "                       max_len=max_len,\n",
    "                       device=device)\n",
    "            else:\n",
    "                model_out = greedy_decode(model=model, \n",
    "                                      source_tokens=encoder_input,\n",
    "                                      source_mask=encoder_mask,\n",
    "                                      tokenizer_src=tokenizer_src,\n",
    "                                      tokenizer_trg=tokenizer_trg,\n",
    "                                      max_len=max_len,\n",
    "                                      device=device)\n",
    "\n",
    "            source_text = batch[\"src_text\"][0]\n",
    "            target_text = batch[\"trg_text\"][0]\n",
    "            model_out_text = tokenizer_trg.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            source_texts.append(source_text)\n",
    "            expected.append(target_text)\n",
    "            predicted.append(model_out_text)\n",
    "            \n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "\n",
    "            if count == num_examples:\n",
    "                print_msg('-'*console_width)\n",
    "                break\n",
    "\n",
    "    evaluate_model_outputs(predicted=predicted,\n",
    "                           expected=expected,\n",
    "                           global_step=global_step,\n",
    "                           writer=writer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('evaluate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate/evaluate_model_outputs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate/evaluate_model_outputs.py\n",
    "import torchmetrics\n",
    "import wandb\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "from char_error_rate import char_error_rate_wandb, char_error_rate_tb\n",
    "from word_error_rate import word_error_rate_wandb, word_error_rate_tb\n",
    "from belu_score import belu_score_wandb, belu_score_tb\n",
    "\n",
    "def evaluate_model_outputs(predicted: List,\n",
    "                           expected: List,\n",
    "                           global_step: int,\n",
    "                           writer: tensorboard,\n",
    "                           wandb: bool = False):\n",
    "    \"\"\"\n",
    "    Function that evaluates model outputs through diffetent metrics:\n",
    "        * Character error rate\n",
    "        * Word error rate\n",
    "        * BELU score\n",
    "    \n",
    "    the results are shown in 2 formats: \n",
    "        * tensorboard\n",
    "        * weights&biases \n",
    "    \n",
    "    Args:\n",
    "        predicted: List\n",
    "            Model output texts \n",
    "        \n",
    "        expected: List\n",
    "            True output texts\n",
    "        \n",
    "        global_step: int\n",
    "            step of trainging\n",
    "\n",
    "        wandb: bool\n",
    "            True to log in wegiths and biases\n",
    "        \n",
    "        writer: tensorboard\n",
    "            writer to show results\n",
    "            \n",
    "    Example:\n",
    "        evaluate_model_outputs(pred, expect, 5, writer, True)\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    \n",
    "    if wandb:\n",
    "        char_error_rate_wandb(predicted=predicted,\n",
    "                              expected=expected,\n",
    "                              global_step=global_step)\n",
    "        \n",
    "        word_error_rate_wandb(predicted=predicted,\n",
    "                              expected=expected,\n",
    "                              global_step=global_step)\n",
    "        \n",
    "        belu_score_wandb(predicted=predicted,\n",
    "                              expected=expected,\n",
    "                              global_step=global_step)\n",
    "    if writer:\n",
    "        char_error_rate_tb(writer=writer,\n",
    "                           predicted=predicted,\n",
    "                           expected=expected,\n",
    "                           global_step=global_step)\n",
    "        \n",
    "        word_error_rate_tb(writer=writer,\n",
    "                           predicted=predicted,\n",
    "                           expected=expected,\n",
    "                           global_step=global_step)\n",
    "        \n",
    "        belu_score_tb(writer=writer,\n",
    "                           predicted=predicted,\n",
    "                           expected=expected,\n",
    "                           global_step=global_step)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate/char_error_rate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate/char_error_rate.py\n",
    "import torchmetrics\n",
    "import wandb\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "def char_error_rate_wandb(predicted: List,\n",
    "                          expected: List,\n",
    "                          global_step: int):\n",
    "    \"\"\"\n",
    "    Model output evaluation function that calculates \n",
    "    the rate of mispredicted cahrecters in the sequance.\n",
    "\n",
    "    The results are logged in weigths and biases format.\n",
    "\n",
    "    Args:\n",
    "        predicted: List\n",
    "            Model output texts \n",
    "        \n",
    "        expected: List\n",
    "            True output texts\n",
    "        \n",
    "        global_step: int\n",
    "            step of trainging\n",
    "    \n",
    "    Example: \n",
    "        char_error_rate_wandb(pred, expect, 5)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Compute the char error rate \n",
    "    metric = torchmetrics.CharErrorRate()\n",
    "    cer = metric(predicted, expected)\n",
    "    wandb.log({'validation/cer': cer, 'global_step': global_step})\n",
    "\n",
    "\n",
    "def char_error_rate_tb(writer: tensorboard,\n",
    "                       predicted: List,\n",
    "                       expected: List,\n",
    "                       global_step: int):\n",
    "    \"\"\"\n",
    "    Model output evaluation function that calculates \n",
    "    the rate of mispredicted cahrecters in the sequance.\n",
    "\n",
    "    The results are shown in tensorboard.\n",
    "\n",
    "    Args:\n",
    "        predicted: List\n",
    "            Model output texts \n",
    "        \n",
    "        expected: List\n",
    "            True output texts\n",
    "        \n",
    "        global_step: int\n",
    "            step of trainging\n",
    "    \n",
    "    Example: \n",
    "        char_error_rate_tb(writer, pred, expect, 5)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Compute the char error rate \n",
    "    metric = torchmetrics.CharErrorRate()\n",
    "    cer = metric(predicted, expected)\n",
    "    writer.add_scalar('validation cer', cer, global_step)\n",
    "    writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate/word_error_rate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate/word_error_rate.py\n",
    "import torchmetrics\n",
    "import wandb\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "def word_error_rate_wandb(predicted: List,\n",
    "                          expected: List,\n",
    "                          global_step: int):\n",
    "    \"\"\"\n",
    "    Model output evaluation function that calculates \n",
    "    the rate of mispredicted words in the sequance.\n",
    "\n",
    "    The results are logged in weigths and biases format.\n",
    "\n",
    "    Args:\n",
    "        predicted: List\n",
    "            Model output texts \n",
    "        \n",
    "        expected: List\n",
    "            True output texts\n",
    "        \n",
    "        global_step: int\n",
    "            step of trainging\n",
    "    \n",
    "    Example: \n",
    "        word_error_rate_wandb(pred, expect, 5)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Compute the word error rate\n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric(predicted, expected)\n",
    "    wandb.log({'validation/wer': wer, 'global_step': global_step})\n",
    "\n",
    "\n",
    "\n",
    "def word_error_rate_tb(writer: tensorboard,\n",
    "                       predicted: List,\n",
    "                       expected: List,\n",
    "                       global_step: int):\n",
    "    \"\"\"\n",
    "    Model output evaluation function that calculates \n",
    "    the rate of mispredicted words in the sequance.\n",
    "\n",
    "    The results are shown in tensorboard.\n",
    "\n",
    "    Args:\n",
    "        predicted: List\n",
    "            Model output texts \n",
    "        \n",
    "        expected: List\n",
    "            True output texts\n",
    "        \n",
    "        global_step: int\n",
    "            step of trainging\n",
    "    \n",
    "    Example: \n",
    "        word_error_rate_tb(writer, pred, expect, 5)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Compute the word error rate\n",
    "    metric = torchmetrics.WordErrorRate()\n",
    "    wer = metric(predicted, expected)\n",
    "    writer.add_scalar('validation wer', wer, global_step)\n",
    "    writer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate/belu_score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate/belu_score.py\n",
    "import torchmetrics\n",
    "import wandb\n",
    "from torch.utils import tensorboard\n",
    "\n",
    "def belu_score_wandb(predicted: List,\n",
    "                     expected: List,\n",
    "                     global_step: int):\n",
    "    \"\"\"\n",
    "    Model output evaluation function that calculates \n",
    "    the rate of mispredicted the BLUE score in the sequance.\n",
    "\n",
    "     BLEU (Bilingual Evaluation Understudy) is \n",
    "     a score used to evaluate the translations performed by a machine translator.\n",
    "\n",
    "    The results are logged in weigths and biases format.\n",
    "\n",
    "    Args:\n",
    "        predicted: List\n",
    "            Model output texts \n",
    "        \n",
    "        expected: List\n",
    "            True output texts\n",
    "        \n",
    "        global_step: int\n",
    "            step of trainging\n",
    "    \n",
    "    Example: \n",
    "        belu_score_wandb(pred, expect, 5)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Compute the BLEU metric\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric(predicted, expected)\n",
    "    wandb.log({'validation/BLEU': bleu, 'global_step': global_step})\n",
    "\n",
    "\n",
    "\n",
    "def belu_score_tb(writer: tensorboard,\n",
    "                  predicted: List,\n",
    "                  expected: List,\n",
    "                  global_step: int):\n",
    "    \"\"\"\n",
    "    Model output evaluation function that calculates \n",
    "    the rate of mispredicted the BLUE score in the sequance.\n",
    "\n",
    "     BLEU (Bilingual Evaluation Understudy) is \n",
    "     a score used to evaluate the translations performed by a machine translator.\n",
    "\n",
    "    The results are shown in tensorboard.\n",
    "\n",
    "    Args:\n",
    "        predicted: List\n",
    "            Model output texts \n",
    "        \n",
    "        expected: List\n",
    "            True output texts\n",
    "        \n",
    "        global_step: int\n",
    "            step of trainging\n",
    "    \n",
    "    Example: \n",
    "        belu_score_tb(writer, pred, expect, 5)\n",
    "    \n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    # Compute the BLEU metric\n",
    "    metric = torchmetrics.BLEUScore()\n",
    "    bleu = metric(predicted, expected)\n",
    "    writer.add_scalar('validation BLEU', bleu, global_step)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.py\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def get_config():\n",
    "    \"\"\"\n",
    "    Function that returns a configruation of model training\n",
    "\n",
    "    Returns: Dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'dataset_name': 'yhavinga/ccmatrix',\n",
    "        'batch_size': 8,\n",
    "        'num_epochs': 20,\n",
    "        'lr': 10**-4,\n",
    "        'seq_len': 350,\n",
    "        'd_model': 512,\n",
    "        'd_ff': 2048,\n",
    "        'number_of_layers': 6,\n",
    "        'number_of_heads': 8,\n",
    "        'lang_src': 'en',\n",
    "        'lang_tgt': 'ar',\n",
    "        'model_folder': 'weights',\n",
    "        'model_basename': 'tmodel_',\n",
    "        'preload': 'latest',\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "    }\n",
    "\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    \"\"\"\n",
    "    Function returns the weights file path \n",
    "    \"\"\"\n",
    "    model_folder = f\"{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}{epoch}.pth\"\n",
    "    return str(Path('.') / model_folder / model_filename)\n",
    "\n",
    "# Find the latest weights file in the weights folder\n",
    "def latest_weights_file_path(config):\n",
    "    \"\"\"\n",
    "    Function returns the latest weights file path \n",
    "    \"\"\"\n",
    "    model_folder = f\"{config['model_folder']}\"\n",
    "    model_filename = f\"{config['model_basename']}*\"\n",
    "    weights_files = list(Path(model_folder).glob(model_filename))\n",
    "    if len(weights_files) == 0:\n",
    "        return None\n",
    "    weights_files.sort()\n",
    "    return str(weights_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train/save_model_state.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/save_model_state.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_weights_file_path\n",
    "from typing import Dict\n",
    "from model.Transformer import Transformer\n",
    "def save_model_state(model: Transformer,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     global_step: int,\n",
    "                     config: Dict,\n",
    "                     epoch: int) -> None:\n",
    "    \"\"\"\n",
    "    function saves the model state, optimizer state, and the global_step for each epoch\n",
    "\n",
    "    Args:\n",
    "        model: Transformer\n",
    "            model to save its state\n",
    "        \n",
    "        optimizer: torch.optim.Optimizer\n",
    "            optimizer to save its state\n",
    "        \n",
    "        global_state: int\n",
    "\n",
    "        config: Dict\n",
    "        epoch: int\n",
    "    \"\"\"\n",
    "     # Save the model at the end of every epoch\n",
    "    model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'global_step': global_step\n",
    "    }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train/train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/train_model.py\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "from model.build_transformer import build_transformer\n",
    "from train.run_validation import run_validation\n",
    "from train.save_model_state import save_model_state\n",
    "from config import get_config, get_weights_file_path, latest_weights_file_path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Dict\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(config: Dict,\n",
    "                writer_: bool,\n",
    "                wandb_: bool):\n",
    "    \"\"\"\n",
    "    function used to make instance of the model and train it to the given dataset\n",
    "\n",
    "    Args:\n",
    "        config: Dict\n",
    "            A dictonary of all neccsary vlaues \n",
    "        writer_: bool \n",
    "            True show results on tensorboard\n",
    "        wandb_: bool\n",
    "            True log results on weights&biases\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    # Define the device through device-agnostic code\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    \n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Make sure the weights folder exists\n",
    "    Path(f\"{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # load the training and validation dataloaders and tokenizers \n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_trg = dataset_loader(conf=config)\n",
    "\n",
    "    # Instaniate the Transformer model\n",
    "    model = build_transformer(src_vocab_size=tokenizer_src.get_vocab_size(),\n",
    "                              trg_vocab_size=tokenizer_trg.get_vocab_size(),\n",
    "                              src_seq_len=config['seq_len'],\n",
    "                              trg_seq_len=config['seq_len'],\n",
    "                              d_model=config['d_model'],\n",
    "                              N=config['number_of_layers'],\n",
    "                              h=config['number_of_heads'],\n",
    "                              dropout=config['dropout'],\n",
    "                              d_ff=config['d_ff'])\n",
    "    \n",
    "    \n",
    "    # Optimimzer to optimize the weights \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=config['lr'],\n",
    "                                 eps=1e-9)\n",
    "\n",
    "    # Loss function to calculate the loss (ignore the padding token from the loss calculations) (smoothing to add bit of randomness)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'),\n",
    "                                  label_smoothing=0.1).to(device)\n",
    "    \n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = None\n",
    "    if preload == 'latest':\n",
    "        model_filename = latest_weights_file_path(config)\n",
    "    elif preload:\n",
    "        get_weights_file_path(config, preload)\n",
    "    \n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "        del state\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    if writer_:\n",
    "\n",
    "        # Tensorboard writer to show summaries of the training \n",
    "        writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    if wandb_:\n",
    "        # define our custom x axis metric\n",
    "        wandb.define_metric(\"global_step\")\n",
    "        # define which metrics will be plotted against it\n",
    "        wandb.define_metric(\"validation/*\", step_metric=\"global_step\")\n",
    "        wandb.define_metric(\"train/*\", step_metric=\"global_step\")\n",
    "\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        # clear the GPU memory \n",
    "        torch.cuda.empty_cache()\n",
    "        # put the model in training mode\n",
    "        model.train()\n",
    "        # create tqdm bar indicator\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "\n",
    "        for batch in batch_iterator:\n",
    "\n",
    "            encoder_input = batch['encoder_input'].to(device) # (batch, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (batch, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (batch, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (batch, 1, seq_len, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(src=encoder_input,\n",
    "                                          src_mask=encoder_mask) # (B, seq_len, d_model)\n",
    "            \n",
    "            decoder_output = model.decode(encoder_output=encoder_output,\n",
    "                                          src_mask=encoder_mask,\n",
    "                                          trg=decoder_input,\n",
    "                                          trg_mask=decoder_mask) # (B, seq_len, d_model)\n",
    "            \n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_trg.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log the loss\n",
    "            if writer_:\n",
    "                writer.add_scalar('train loss', loss.item(), global_step)\n",
    "                writer.flush()\n",
    "            \n",
    "            if wandb_:\n",
    "                wandb.log({'train/loss': loss.item(), 'global_step': global_step})\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model=model,\n",
    "                       validation_ds=val_dataloader,\n",
    "                       tokenizer_src=tokenizer_src,\n",
    "                       tokenizer_trg=tokenizer_trg,\n",
    "                       max_len=config['seq_len'],\n",
    "                       device=device,\n",
    "                       print_msg=lambda msg: batch_iterator.write(msg),\n",
    "                       global_step=global_step,\n",
    "                       writer=writer,\n",
    "                       wandb_=wandb_)\n",
    "        \n",
    "        # Save the model at the end of every epoch\n",
    "        save_model_state(model=model,\n",
    "                         optimizer=optimizer,\n",
    "                         global_step=global_step,\n",
    "                         config=config,\n",
    "                         epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Inference and Searching\n",
    "\n",
    "In this section we use the pretrained model to make infrerences.\n",
    "\n",
    "This Section contains 2 main functions:\n",
    "\n",
    "* Translate\n",
    "* Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting translate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile translate.py\n",
    "from pathlib import Path\n",
    "from config import get_config, latest_weights_file_path\n",
    "from model.build_transformer import build_transformer\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_dataset\n",
    "from dataset.BilingualDataset import BilingualDataset\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "def translate(sentence: str):\n",
    "    \"\"\"\n",
    "    Function used to make predictions on custom inputs. \n",
    "    The function takes a sentence in English and output Arabic translation of it.\n",
    "\n",
    "    Args:\n",
    "        sentence: str\n",
    "            English sentence to be translated\n",
    "    \n",
    "    Example: \n",
    "        arabic = translate('I want to go to school')\n",
    "\n",
    "    Returns: \n",
    "        out: str\n",
    "            Arabic translation\n",
    "    \"\"\"\n",
    "    # Define the device, tokenizers, and model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    config = get_config()\n",
    "\n",
    "    tokenizer_src = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_src']))))\n",
    "    tokenizer_trg = Tokenizer.from_file(str(Path(config['tokenizer_file'].format(config['lang_trg']))))\n",
    "    \n",
    "    model = build_transformer(src_vocab_size=tokenizer_src.get_vocab_size(),\n",
    "                              trg_vocab_size=tokenizer_trg.get_vocab_size(),\n",
    "                              src_seq_len=config[\"seq_len\"],\n",
    "                              trg_seq_len=config['seq_len'],\n",
    "                              d_model=config['d_model']).to(device)\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model_filename = latest_weights_file_path(config)\n",
    "    state = torch.load(model_filename)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "    # if the sentence is a number use it as an index to the test set\n",
    "    label = \"\"\n",
    "    if type(sentence) == int or sentence.isdigit():\n",
    "        id = int(sentence)\n",
    "        ds = load_dataset(f\"{config['dataset_name']}\", f\"{config['lang_src']}-{config['lang_trg']}\", split='all')\n",
    "        ds = BilingualDataset(ds, tokenizer_src, tokenizer_trg, config['seq_len'])\n",
    "        sentence = ds[id]['src_text']\n",
    "        label = ds[id][\"trg_text\"]\n",
    "    seq_len = config['seq_len']\n",
    "\n",
    "    # translate the sentence\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Precompute the encoder output and reuse it for every generation step\n",
    "        source = tokenizer_src.encode(sentence)\n",
    "        source = torch.cat(\n",
    "            tensors=[\n",
    "            torch.tensor([tokenizer_src.token_to_id('[SOS]')], dtype=torch.int64), \n",
    "            torch.tensor(source.ids, dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[EOS]')], dtype=torch.int64),\n",
    "            torch.tensor([tokenizer_src.token_to_id('[PAD]')] * (seq_len - len(source.ids) - 2), dtype=torch.int64)\n",
    "        ], dim=0).to(device)\n",
    "\n",
    "        source_mask = (source != tokenizer_src.token_to_id('[PAD]')).unsqueeze(0).unsqueeze(0).int().to(device)\n",
    "        \n",
    "        encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "        # Initialize the decoder input with the sos token\n",
    "        decoder_input = torch.empty(1, 1).fill_(tokenizer_trg.token_to_id('[SOS]')).type_as(source).to(device)\n",
    "\n",
    "        # Print the source sentence and target start prompt\n",
    "        if label != \"\": print(f\"{f'ID: ':>12}{id}\") \n",
    "        print(f\"{f'SOURCE: ':>12}{sentence}\")\n",
    "        if label != \"\": print(f\"{f'TARGET: ':>12}{label}\") \n",
    "        print(f\"{f'PREDICTED: ':>12}\", end='')\n",
    "\n",
    "        # Generate the translation word by word\n",
    "        while decoder_input.size(1) < seq_len:\n",
    "            # build mask for target and calculate output\n",
    "            decoder_mask = torch.triu(torch.ones((1, decoder_input.size(1), decoder_input.size(1))), diagonal=1).type(torch.int).type_as(source_mask).to(device)\n",
    "            out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "            # project next token\n",
    "            prob = model.project(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            decoder_input = torch.cat(\n",
    "                tensors=[\n",
    "                    decoder_input,\n",
    "                    torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)\n",
    "                ], \n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # print the translated word\n",
    "            print(f\"{tokenizer_trg.decode([next_word.item()])}\", end=' ')\n",
    "\n",
    "            # break if we predict the end of sentence token\n",
    "            if next_word == tokenizer_trg.token_to_id('[EOS]'):\n",
    "                break\n",
    "\n",
    "    # convert ids to tokens\n",
    "    return tokenizer_trg.decode(decoder_input[0].tolist())\n",
    "    \n",
    "#read sentence from argument\n",
    "translate(sys.argv[1] if len(sys.argv) > 1 else \"I am not a very good a student.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Device name: NVIDIA GeForce GTX 1650\n",
      "Device memory: 3.99969482421875 GB\n",
      "Folder exists. Loading the dataset from the disk...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f14c5fef6154f70a78a53bba4c60785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset from disk:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:473: UserWarning: Length of split at index 0 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calculate_max_seq_len() got an unexpected keyword argument 'lang_src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_model\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwandb_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Academic\\Code\\Transformer\\Englishiano-Transformer\\train\\train_model.py:44\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(config, writer_, wandb_)\u001b[0m\n\u001b[0;32m     41\u001b[0m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_folder\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# load the training and validation dataloaders and tokenizers \u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m train_dataloader, val_dataloader, tokenizer_src, tokenizer_trg \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Instaniate the Transformer model\u001b[39;00m\n\u001b[0;32m     47\u001b[0m model \u001b[38;5;241m=\u001b[39m build_transformer(src_vocab_size\u001b[38;5;241m=\u001b[39mtokenizer_src\u001b[38;5;241m.\u001b[39mget_vocab_size(),\n\u001b[0;32m     48\u001b[0m                           trg_vocab_size\u001b[38;5;241m=\u001b[39mtokenizer_trg\u001b[38;5;241m.\u001b[39mget_vocab_size(),\n\u001b[0;32m     49\u001b[0m                           src_seq_len\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m                           dropout\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     55\u001b[0m                           d_ff\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_ff\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32md:\\Academic\\Code\\Transformer\\Englishiano-Transformer\\dataset\\dataset_loader.py:62\u001b[0m, in \u001b[0;36mdataset_loader\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     59\u001b[0m val_ds_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset_raw) \u001b[38;5;241m-\u001b[39m train_ds_size\n\u001b[0;32m     60\u001b[0m train_ds_raw, val_ds_raw \u001b[38;5;241m=\u001b[39m random_split(dataset\u001b[38;5;241m=\u001b[39mdataset_raw, lengths\u001b[38;5;241m=\u001b[39m[train_ds_size, val_ds_size])\n\u001b[1;32m---> 62\u001b[0m conf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_max_seq_len\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msrc_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_src\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtrg_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_trg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlang_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang_src\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mlang_trg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang_trg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m BilingualDataset(datasaet\u001b[38;5;241m=\u001b[39mtrain_ds_raw,\n\u001b[0;32m     70\u001b[0m                                  src_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_src,\n\u001b[0;32m     71\u001b[0m                                  trg_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_trg,\n\u001b[0;32m     72\u001b[0m                                  seq_len\u001b[38;5;241m=\u001b[39mconf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     73\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m BilingualDataset(datasaet\u001b[38;5;241m=\u001b[39mval_ds_raw,\n\u001b[0;32m     74\u001b[0m                                  src_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_src,\n\u001b[0;32m     75\u001b[0m                                  trg_tokenizer\u001b[38;5;241m=\u001b[39mtokenizer_trg,\n\u001b[0;32m     76\u001b[0m                                  seq_len\u001b[38;5;241m=\u001b[39mconf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: calculate_max_seq_len() got an unexpected keyword argument 'lang_src'"
     ]
    }
   ],
   "source": [
    "from config import get_config\n",
    "cfg = get_config()\n",
    "cfg['batch_size'] = 6\n",
    "cfg['preload'] = None\n",
    "cfg['num_epochs'] = 15\n",
    "\n",
    "from train.train_model import train_model\n",
    "\n",
    "train_model(config=cfg,\n",
    "            writer_=True,\n",
    "            wandb_=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The system cannot find the file specified. (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_loader\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuild_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_transformer\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtranslate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m translate\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Define the device\u001b[39;00m\n\u001b[0;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Academic\\Code\\Transformer\\Englishiano-Transformer\\translate.py:98\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_trg\u001b[38;5;241m.\u001b[39mdecode(decoder_input[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m#read sentence from argument\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI am not a very good a student.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Academic\\Code\\Transformer\\Englishiano-Transformer\\translate.py:20\u001b[0m, in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[0;32m     18\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m---> 20\u001b[0m tokenizer_src \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokenizer_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang_src\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m tokenizer_trg \u001b[38;5;241m=\u001b[39m Tokenizer\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;28mstr\u001b[39m(Path(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlang_trg\u001b[39m\u001b[38;5;124m'\u001b[39m]))))\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m build_transformer(src_vocab_size\u001b[38;5;241m=\u001b[39mtokenizer_src\u001b[38;5;241m.\u001b[39mget_vocab_size(),\n\u001b[0;32m     24\u001b[0m                           trg_vocab_size\u001b[38;5;241m=\u001b[39mtokenizer_trg\u001b[38;5;241m.\u001b[39mget_vocab_size(),\n\u001b[0;32m     25\u001b[0m                           src_seq_len\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     26\u001b[0m                           trg_seq_len\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq_len\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     27\u001b[0m                           d_model\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mException\u001b[0m: The system cannot find the file specified. (os error 2)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from config import get_config, latest_weights_file_path\n",
    "from train.run_validation import run_validation\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "from model.build_transformer import build_transformer\n",
    "from translate import translate\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "config = get_config()\n",
    "\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_trg = dataset_loader(conf=config)\n",
    "\n",
    "model = build_transformer(src_vocab_size=tokenizer_src.get_vocab_size(),\n",
    "                          trg_vocab_size=tokenizer_trg.get_vocab_size(),\n",
    "                          src_seq_len=config[\"seq_len\"],\n",
    "                          trg_seq_len=config[\"seq_len\"],\n",
    "                          d_model=config['d_model']).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = latest_weights_file_path(config)\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "\n",
    "run_validation(model, val_dataloader, tokenizer_src, tokenizer_trg, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=4)\n",
    "\n",
    "t = translate(\"Why do I need to translate this?\")\n",
    "\n",
    "print(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
